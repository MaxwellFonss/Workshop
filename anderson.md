Although the creation of big data has substantially increased the accuracy of empirical studies, I believe that the limitations of this new field of study prevent it from becoming a 'forth paradigm' that eliminates traditional theory. 

Anderson does bring up many compelling examples of how big data has outpreformed traditional models, such as google's language translation algorithms or targeted ad campaigns. However, while big data may be able to evaluate the current world, it still lacks the ability to make long term predictions or offer explanations for it's models. For instance, google's ad algorithms cannot determine why customers decided to click on an ad, so it would be difficult to draw any conclusions from the algorithm outside of what is what specifically designed to evalute.

This is especially important in research fields that are less empirical, such as sociology or economics. If we cannot explain our models in a meaningful way, they cannot be applied in different applications or adapted for the future. For instance, our current understanding of growth within macroeconomics is still and will likely continue to be theory based, since some subjects will always require infinite amounts of data or human critical thinking.
